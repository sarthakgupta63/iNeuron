{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**.What are deep learning modelling techniques to handle textual data?\n",
    "\n",
    "**Answer**. \n",
    "\n",
    "Genral Text preprocessing Techniques -\n",
    "1. Tokenization\n",
    "2. Removing Special Characters & Numbers\n",
    "3. Removing Stop Words\n",
    "4. Stemming/Lemmatization\n",
    "\n",
    "Deep Learning Specific -\\\n",
    "Embedding/Word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**.What is RNN?Why we use them?\n",
    "\n",
    "**Answer**.\n",
    "\n",
    "RNN is an abbreviation for Recurrent Neural Networks. These are specific types of Artificial Neural Networks which takes along the the information from the previous layers into the upcoming layers (layers more in-depth).\\\n",
    "RNNs are specially very helpful in modelling deep learning models for sequential data problems, like textual data problems (NLP) and Time-Series problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**.State down few problems in RNN?\n",
    "\n",
    "**Answer**.\\\n",
    "RNNs generally face the problem of \n",
    "1. Vanishing Gradients\n",
    "2. Exploding Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**.What is vanishing gradient and gradient explosion?\n",
    "\n",
    "**Answer**.\\\n",
    "Exploding Gradients\\\n",
    "    When the derivatives are large, as we do backpropagation these gradients incearse very rapidly and sometimes at some point become very large which then impaires the learning of the model without considering previous values and other parameters. This Phenomenon is called Gradient Explosion/Exploding gradient.\n",
    "\n",
    "Vanishing Gradients\\\n",
    "    When the derivatives are small, as we do backpropagation these gradients decrease very rapidly and sometimes at some point become equal or nearly equal to zero, which then stops the further learning process of a model. This Phenomenon is called vanishing gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**.What is LSTM?\n",
    "\n",
    "**Answer**.\\\n",
    "LSTM is short for Long-Short Term Memory. It was designed to overcome the vanishing gradient problem in RNNs.\\\n",
    "LSTMs are basically composed of 4 components - cell, input gate, output gate, update gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**.Benifits of LSTM over RNN\n",
    "\n",
    "**Answer**.\\\n",
    "1. Solves Vanishing Gradient Problem\n",
    "2. Can focus on the information that is more relavant for long time (input series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
